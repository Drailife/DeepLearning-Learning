## 七、实际应用部分

### 1. 全连接神经网络

#### Ⅰ 实现线性回归模型

参见全连接层基础知识

- 线性回归模型

```python
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
# 数据
x_data = torch.Tensor([[1.], [2.], [3.], [4.]])
y_data = torch.Tensor([[2.0], [4.0], [6.0], [8.0]])


# 线性回归网络模型
class LinearModule(nn.Module):
    def __init__(self):
        super(LinearModule, self).__init__()
        self.linear = nn.Linear(1, 1)

    def forward(self, x):
        y_pred = self.linear(x)
        return y_pred


MyModele = LinearModule()  # 模型的实例化对象
criterion = nn.SmoothL1Loss(size_average=False)  # 损失器
optimizer = torch.optim.ASGD(MyModele.parameters(), lr=0.001)
loss_list = []
for epoch in range(999):
    y_pred = MyModele(x_data)  # 计算预测值
    loss = criterion(y_data, y_pred)  # 计算损失值
    loss_list.append(loss.item())
    print(epoch, loss.item())

    optimizer.zero_grad()  # 梯度清零
    loss.backward()  # 反向传播
    optimizer.step()  # 梯度更新
    # print('w = ', MyModele.linear.weight.item())
    # print('b = ', MyModele.linear.bias.item())

print('w = ', MyModele.linear.weight.item())
print('b = ', MyModele.linear.bias.item())

x_test = torch.Tensor([[20.0]])
y_test = MyModele(x_test)
print('y_pred = ', y_test)
plt.plot(loss_list)
plt.show()
```

#### Ⅱ 糖尿病检测

效果很垃圾很垃圾很垃圾很垃圾，白等那么久了，艹………………………………………………

```python
import time

import numpy as np
import torch.nn as nn
import torch.utils.data
from torch.utils.data import Dataset


# 定义自己的Dataset
class MyDataset(Dataset):
    def __init__(self, datasname):
        super(MyDataset, self).__init__()
        xy_datas = np.loadtxt(datasname, delimiter=',', dtype=np.float32)  # 读取csv中的文本数据
        self.length = xy_datas.shape[0]  # 得到数据的数量
        # print(xy_datas)
        # print(xy_datas.shape)   # (759, 9)
        self.x_datas = xy_datas[:, :-1]  # 表示截取除去最后一列的所有数据，若为[: , :-2]则为获取除去最后两列的所有数据 [行 ， 列]
        self.y_datas = xy_datas[:, [-1]]  # 表示获取最后一列的所有数据 [: , [n]] 表示获取第n列的数据

    def __getitem__(self, item):
        return self.x_datas[item], self.y_datas[item]

    def __len__(self):
        return self.length


# 定义自己的线性回归模型
class MyModle(nn.Module):
    def __init__(self):
        super(MyModle, self).__init__()
        self.linear1 = nn.Linear(8, 6)
        self.linear2 = nn.Linear(6, 4)
        self.linear3 = nn.Linear(4, 1)
        self.sigmoid = nn.Sigmoid()

    # 重写前向传播函数
    def forward(self, x):
        x = self.sigmoid(self.linear1(x))
        x = self.sigmoid(self.linear2(x))
        x = self.sigmoid(self.linear3(x))
        return x


dataset = MyDataset('.\MyDatas\diabetes.csv.gz')	#数据地址https://pan.baidu.com/disk/main#/index?category=all&path=%2FLearning%20materials%2FPyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5
dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=16, shuffle=True)
my_modle = MyModle()  # 创建一个模型的实例化对象
criterion = nn.BCELoss()  # 二分类问题，选用Bceloss 二分类交叉熵损失函数计算损失值
optimizer = torch.optim.Adam(params=my_modle.parameters(), lr=0.01)  # 选用随机梯度下降算法定义优化器

train_epoch = 10000
my_modle.train()
if __name__ == '__main__':
    for epoch in range(train_epoch):
        print('-----', epoch)
        for i, datas in enumerate(dataloader):
            # print(datas)
            # 1. Prepared datas
            data, lables = datas
            # 2. Forward
            lable_pred = my_modle(data)
            loss = criterion(lables, lable_pred)  # 计算损失值
            print('\t', loss.item())
            # 3. Backward
            loss.backward()  # 反向传播
            optimizer.zero_grad()  # 梯度清零
            # 4. Update
            optimizer.step()  # 梯度更新

```

#### Ⅲ MNIST手写字体识别

手写字体图片大小为 28*28

```python
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

Device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print('你正在使用的为： ', Device)
# 加载数据集
transform = transforms.Compose([transforms.ToTensor(),  # 将数据变成[0,1] 直接除以255得到的
                                transforms.Normalize(mean=(0.1307,), std=(0.3081,))
                                # 将数据规划到【-1， 1】 计算方法  x = (x-mean)/std
                                # mean 和 std是前人经过计算得到的
                                ])
trian_dataset = torchvision.datasets.MNIST(root='.\MyDatas\Train_MNIST手写字体',
                                           train=True,
                                           download=True,
                                           transform=transform)
train_dataloader = torch.utils.data.DataLoader(dataset=trian_dataset,
                                               batch_size=64,
                                               shuffle=True)

test_dataset = torchvision.datasets.MNIST(root='.\MyDatas\Test_MNIST手写字体',
                                          train=False,
                                          download=True,
                                          transform=transform)
test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset,
                                              batch_size=64,
                                              shuffle=True)


# 定义自己的模型
class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.linear1 = nn.Linear(784, 512)
        self.linear2 = nn.Linear(512, 256)
        self.linear3 = nn.Linear(256, 128)
        self.linear4 = nn.Linear(128, 64)
        self.linear5 = nn.Linear(64, 10)
        self.relu = torch.nn.ReLU()

    def forward(self, x):
        x = x.view(-1, 784)  # -1 代表自动计算 原来为64*1*28*28 现在为64*784
        x = self.relu(self.linear1(x))
        x = self.relu(self.linear2(x))
        x = self.relu(self.linear3(x))
        x = self.relu(self.linear4(x))
        x = self.linear5(x)
        return x


model = MyModel()
model = model.to(Device)
criterion = torch.nn.CrossEntropyLoss()  # 交叉熵损失函数
criterion = criterion.to(Device)
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)  # 随机梯度下降

train_epochs = 70
model.train()
loss_plot = []
acc_plot = []
for epoch in range(train_epochs):
    each_epoch_loss = 0
    for batch_idx, (datas, lables) in enumerate(train_dataloader):
        datas = datas.to(Device)
        lables = lables.to(Device)
        lables_pre = model(datas)
        each_ite_loss = criterion(lables_pre, lables)
        each_epoch_loss += each_ite_loss.item()
        optimizer.zero_grad()  # 梯度清零
        each_ite_loss.backward()  # 反向传播
        optimizer.step()  # 梯度更新
    loss_plot.append(each_epoch_loss)
    print('epoch: ', epoch, 'loss: ', each_epoch_loss)
    correct = 0
    total = 0
    with torch.no_grad():
        for data in test_dataloader:
            images, labels = data
            images = images.to(Device)
            labels = labels.to(Device)
            outputs = model(images)
            predicted = torch.max(outputs, dim=1)[1]
            total += labels.size(0)
            correct += (predicted == labels).sum().item()   # 计算正确率

        acc_plot.append(100 * correct / total)
        print('Accuracy on test set: %d %%' % (100 * correct / total))
plt.figure(figsize=(8, 4))
plt.subplot(1, 2, 1)
plt.plot(loss_plot)
plt.subplot(1, 2, 2)
plt.plot(acc_plot)
plt.show()

```

## 