## 二、数据装载操作

| 类                               | 功能                                                         |
| -------------------------------- | ------------------------------------------------------------ |
| torch.utils.data.Dataset()       | 表示一个数据集的抽象类，所有的其它数据集都要以它为父类进行数据封装 |
| torch.utils.data.TensorDataset() | 包装数据和目标张量的数据集,父类为Dataset                     |
| torch.utils.data.DataLoader()    | 对数据进行加载                                               |

### 2.1 torch.utils.data.Dataset()

```python
class Dataset(object):
	def  __getitem__(self, index):
		raise NotImplementError
	def __len__(self):
		raise NotImplementError
	def __add__(self, other):
		return ConcatDataset([self, other])

```

torch.utils.data.Dataset表示一个数据集的抽象类，所有的其它数据集都要以它为父类进行数据封装。Dataset的类函数__getitem__和__len__必须要被进行重写

### 2.2 torch.utils.data.TensorDataset()

```python
torch.utils.data.TensorDataset(data_tensor, target_tensor)
        data_tensor : 需要被封装的数据样本
        target_tensor : 需要被封装的数据标签
```

```python
class TensorDataset(Dataset):
    # TensorDataset继承Dataset, 重载了__init__, __getitem__, __len__
    def __init__(self, data_tensor, target_tensor):
        self.data_tensor = data_tensor
        self.target_tensor = target_tensor
    def __getitem__(self, index):
        return self.data_tensor[index], self.target_tensor[index]
    def __len__(self):
        return self.data_tensor.size(0)

```

torch.utils.data.TensorDataset继承父类torch.utils.data.Dataset，不需要对类TensorDataset的函数进行重写

### 2.3 torch.utils.data.DataLoader()

```python
class torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, multiprocessing_context=None)
```

```python
dataset (Dataset): 封装后的数据集。
batch_size (python:int,optional)): 每一批加载的样本量，默认值为1。
shuffle (bool,optional): 设置为True时,每一个epoch重新打乱数据顺序。
sampler (Sampler,optional): 定义在数据集中进行采样的策略，如果被指定，则False必须为shuffle。
batch_sampler (Sampler,optional): 类似sampler，但是一次返回一批索引。互斥有batch_size，shuffle，sampler和drop_last。
num_workers (python:int,optional): 多少个子进程用于数据加载。0表示将在主进程中加载数据,默认值为0。
collate_fn(callable,optional): 合并样本列表以形成张量的小批量。在从地图样式数据集中使用批量加载时使用。
pin_memory (bool,optional): 如果为True，则数据加载器在将张量返回之前将其复制到CUDA固定的内存中。
drop_last (bool,optional): 设置为True，如果数据集大小不能被该批次大小整除则删除最后一个不完整的批次。如果False，数据集的大小不能被批量大小整除，那么最后一个批量将更小，默认值为False。
timeout (numeric,optional): 如果为正，则为从worker收集批次的超时值。应始终为非负数,默认值为0。
worker_init_fn (callable,optional): 如果不是None，则在种子工作之后和数据加载之前，将在每个工作程序子进程上调用此程序，并以工作程序ID作为输入,取值为[0, num_workers - 1]或None。
```

torch.utils.data.DataLoader结合了数据集和取样器，并且可以提供多个线程处理数据集。在训练模型时该类可以将数据进行切分，每次抛出一组数据，直至把所有的数据都抛出

### 2.4 torchvision.datasets.ImageFolder()

文件夹的名称为target，对应的文件夹中内容为数据

```python
dataset=torchvision.datasets.ImageFolder(
                       root, transform=None, 
                       target_transform=None, 
                       loader=<function default_loader>, 
                       is_valid_file=None)
```

- root：图片存储的根目录，即各类别文件夹所在目录的上一级目录。
- transform：对图片进行预处理的操作（函数），原始图片作为输入，返回一个转换后的图片。
- target_transform：对图片类别进行预处理的操作，输入为 target，输出对其的转换。 如果不传该参数，即对 target 不做任何转换，返回的顺序索引 0,1, 2…
- loader：表示数据集加载方式，通常默认加载方式即可。
- is_valid_file：获取图像文件的路径并检查该文件是否为有效文件的函数(用于检查损坏文件)



### 2.5 transforms.ToTensor(）

ToTensor()将shape为`(H, W, C)`的nump.ndarray或img转为shape为`(C, H, W)`的tensor，其将每一个数值归一化到**[0,1]**，其归一化方法比较简单，直接除以255即可

### 2.6 transforms.Normalize()

<img src="D:\Application data(D)\Typora\Image\normalize.png"></img>

```python
transforms.Compose([transforms.ToTensor(),
                    transforms.Normalize(std=(0.5,0.5,0.5),mean=(0.5,0.5,0.5))])
# 则其作用就是先将输入归一化到(0,1)，再使用公式"(x-mean)/std"，将每个元素分布到(-1,1)
```

mean 和 std取值可以总数据中抽样进行计算

### 2.7 transforms.Compose()

一般用Compose将多个处理步骤整合到一起

```python
transform = transforms.Compose([ 
            transforms.ToTensor(), 
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), 
        ]) 

```



### 2.8 加载自己的数据集

```python
class MyDataset(Dataset):
	def __init__(self, root, transform=None):
    	pass
        def __getitem__(self, index):
            # 	倾向于在这里调用transform
    	pass		# 可以使用下标访问
        def __len__(self):
    	pass		# 返回数据集大小
transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize(mean=(0.5,), std=(0.5,))
                                ])
dataset = MyDataset(root='your path', transform=transform)
train_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=2)
# 若使用num_workers 在windows系统下需要加上if __name__ == '__main__':
# 在数据集较小的时候速度会变慢
```

