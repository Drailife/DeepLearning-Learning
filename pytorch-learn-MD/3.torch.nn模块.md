## 三、torch.nn模块

### 3.0 <a href="https://pytorch.org/docs/stable/nn.html#convolution-layers">卷积层</a>

`torch.nn.Conv2d`(*in_channels*, *out_channels*, *kernel_size*, *stride=1*, *padding=0*, *dilation=1*, *groups=1*, *bias=True*, *padding_mode='zeros'*, *device=None*, *dtype=None*)

out_channels为多少就有多少个卷积核



<div align="center"><img src="D:\Application data(D)\Typora\Image\卷积.png"></img></div>

<div align="center"><img src="D:\Application data(D)\Typora\Image\1卷积.png"></img></div>

**1*1卷积的作用是减少计算量**

<div align="center"><img src="D:\Application data(D)\Typora\Image\11卷积.png"></img></div>

### 3.1 <a href="https://pytorch.org/docs/stable/nn.html#pooling-layers">池化层</a>

`torch.nn.MaxPool2d`(*kernel_size*, *stride=None*, *padding=0*, *dilation=1*, *return_indices=False*, *ceil_mode=False*)

<div align="center"><img src="D:\Application data(D)\Typora\Image\maxpool.png"></img></div>

### 3.2 激活函数



### 3.3 循环层



### 3.4 全连接层

在[pytorch](https://so.csdn.net/so/search?q=pytorch&spm=1001.2101.3001.7020)中**的nn.Linear表示线性变换**，官方文档给出的数学计算公式是

<div align="center"><img src="https://private.codecogs.com/gif.latex?y%20%3D%20xA%5ET%20&plus;%20b"></img></div>

```python
class Linear(Module):
    def __init__(self, in_features: int, out_features: int, bias: bool = True) -> None:
```

in_features: size of each input sample，每个输入样本的大小，输入x的**列数**，输入数据[batchsize, in_features]

out_features: size of each output sample ，每个输出样本的大小，线性变换后输出y的列数，输出数据大小是： [batchsize, out_features]

其中x是输入，A是[权值](https://so.csdn.net/so/search?q=权值&spm=1001.2101.3001.7020)，b是偏置，y是输出



```python
import torch.nn as nn
import torch
m = nn.Linear(20, 30)
input = torch.autograd.Variable(torch.randn(128, 20))
output = m(input)
print(output.size())#[128, 30]
```

分析：output.size()=矩阵size(128,20)*矩阵size(20,30)=(128,30)

线性变换不改变输入矩阵x的行数，仅改变**列数**



### 3.5输出大小计算

<div align="center"><img src="D:\Application data(D)\Typora\Image\size_cal.png"></img></div>



