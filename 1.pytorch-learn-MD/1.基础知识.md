## 一.简介

### 1.0 PyTorch使用的数据类型

使用pytorch时需要的默认的数据应该为torch的32位浮点型的张量

```python
import numpy as np
import torch

a = np.array([1, 2, 3])
print(a, a.dtype, sep='\t')
a_tor = torch.from_numpy(a.astype(np.float32))
print(a_tor, a_tor.dtype, sep='\t')

# output
[1 2 3]	int32
tensor([1., 2., 3.])	torch.float32
```



### 1.1  张量初始化

```python
#-------------------------------------------------------------
#              Pytorch 张量初始化
#-------------------------------------------------------------

# 导入 pytorch 库
import torch

# 初始化张量
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
x = torch.tensor([[1,2,3],[4,5,6]],dtype = torch.float32, device = DEVICE, requires_grad = True)

print(x)
print(x.dtype)
print(x.device)
print(x.requires_grad)

# 其它初始化方法
x = torch.rand((2,3)) # rand 均匀分布
print(x)

x = torch.randn((2,3)) # randn 正态分布
print(x)

x = torch.randint(3,10,(2,3)) # randint 均匀分布取整数
print(x)

input = torch.rand((3,3))
x = torch.rand_like(input) # rand_like 用rand来取与input同样维度的矩阵
print(x)

x = torch.zeros((2,3)) # 生成0矩阵
print(x)

x = torch.ones((2,3)) # 生成1矩阵
print(x)

x = torch.eye(3,3) # 生成对角单位1矩阵
print(x)

x = torch.arange(start=0, end=10, step=1) #以step为间隔输出从start到end的张量列表， 注意不包含end值
print(x)

x = torch.linspace(start=0, end=9, steps=11) #返回一个1维张量，包含在区间start和end上均匀间隔的steps的值
print(x)

x = torch.diag(torch.rand(5)) #生成对角矩阵
print(x)

```

###  1.2 N与 P 数据类型转换

```python
# 导入Numpy包
import numpy as np

# 定义一个Ndarray对象
x = np.zeros((2,3))
print(x)

# 从 Numpy的Array 到 Torch的Tensor 的转换
x_torch = torch.from_numpy(x)
print(x_torch)

# 从 Torch的Tensor 到 Numpy的Array 的转换
x_back = x_torch.numpy()
print(x_back)
```

### 1.3 张量的基本数学运算

```python
#-------------------------------------------------------------
#              Pytorch 张量的基本数学运算
#-------------------------------------------------------------

# 导入pytorch
import torch

# 基本数学运算
x = torch.tensor([2,2,2], dtype = torch.float32)
y = torch.tensor([3,4,5], dtype = torch.float32)

# 加法
out = torch.add(x,y)
print(out)

out = x + y
print(out)

# 减法
out = x - y
print(out)

out = torch.sub(x,y)
print(out)

# 除法
out = torch.div(x,y)
print(out)

out = x / y
print(out)

# 乘法
out = torch.mul(x,y)
print(out)

out = x * y
print(out)

# 矩阵乘法
x = torch.rand((2,5))
y = torch.rand((5,3))

out = torch.mm(x,y) # 2*3
print(out)
print(out.shape)

# 批量矩阵乘法
# 批量batch
batch = 16
c1 = 5
c2 = 10
c3 = 20

x1 = torch.rand(batch,c1,c2) #16*5*10
x2 = torch.rand(batch,c2,c3) #16*10*20

out = torch.bmm(x1,x2) #16*5*20
print(out.shape)

#指数
x = torch.tensor([[2,2,2],[2,2,2]])
out = x.pow(3)
print(out)

out = x ** 3
print(out)

#矩阵指数
x = torch.tensor([[1,1],[1,1]]) # 2*2
out = x.matrix_power(2)
print(out) #2*2

out = x.pow(2)
print(out)

# broadcasting
x1 = torch.rand((1,3))
x2 = torch.rand((3,3))

out = x1 - x2
print(out)

x1 = torch.rand((2,2))
x2 = torch.randint(1,10,(2,2))
out = x1.pow(x2)
print(out)

x2 = torch.randint(1,10,(1,2))
out = x1.pow(x2)
print(out)
```

### 1.4 Pytorch使用GPU训练

使用 GPU训练只需要在原来的代码中修改几处就可以了。

<div><img src='https://img-blog.csdnimg.cn/33b6caf7d51e4e318c725f057a09574d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA6aOO5ZC55oiR5Lqm5pWj,size_20,color_FFFFFF,t_70,g_se,x_16'></img></div>

我们有两种方式实现代码在 GPU 上进行训练

- 方法一 .cuda() 

  我们可以通过对网络模型，数据，损失函数这三种变量调用 .cuda() 来在GPU上进行训练

  ```python
  # 将网络模型在gpu上训练
  model = Model()
  model = model.cuda()
  
  # 损失函数在gpu上训练
  loss_fn = nn.CrossEntropyLoss()
  loss_fn = loss_fn.cuda()
  
  # 数据在gpu上训练
  for data in dataloader:                        
  	imgs, targets = data
  	imgs = imgs.cuda()
  	targets = targets.cuda()
  
  ```

- 方法二 .to(device)

  ```python
  device = torch.device("cpu")	# 使用cpu训练
  device = torch.device("cuda")	# 使用gpu训练 
  device = torch.device("cuda:0")	# 当电脑中有多张显卡时，使用第一张显卡
  device = torch.device("cuda:1")	# 当电脑中有多张显卡时，使用第二张显卡
  
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  
  ```

  

### 1.5 维度排列和拼接

在pytorch中维度按照【**batch_size, channel, height,width**】进行排列



#### Ⅰ torch.cat(tensors, dim)

参数

- **tensors** （ *sequence of Tensors*）——任何相同类型的张量的python序列。  提供的非空张量必须具有**相同的形状**。 
- **dim** ( [*int* ](https://docs.python.org/3/library/functions.html#int)*,* *optional* 张量的 维度 连接 

```python
import torch

x = torch.linspace(1, 8, 8)
x = x.reshape(2, 4)
print(x)
print(torch.cat(tensors=[x, x, x], dim=0))	# 在高（列）上来连接
print(torch.cat(tensors=[x, x, x], dim=1))    # 在宽（行）上来连接

# 输出
tensor([[1., 2., 3., 4.],
        [5., 6., 7., 8.]])

tensor([[1., 2., 3., 4.],
        [5., 6., 7., 8.],
        [1., 2., 3., 4.],
        [5., 6., 7., 8.],
        [1., 2., 3., 4.],
        [5., 6., 7., 8.]])

tensor([[1., 2., 3., 4., 1., 2., 3., 4., 1., 2., 3., 4.],
        [5., 6., 7., 8., 5., 6., 7., 8., 5., 6., 7., 8.]])
```

