## 二、数据预处理加载及装载操作

| 类                               | 功能                                                         |
| -------------------------------- | ------------------------------------------------------------ |
| torch.utils.data.Dataset()       | 表示一个数据集的抽象类，所有的其它数据集都要以它为父类进行数据封装 |
| torch.utils.data.TensorDataset() | 包装数据和目标张量的数据集,父类为Dataset                     |
| torch.utils.data.DataLoader()    | 对数据进行加载                                               |

### 2.1 torch.utils.data.Dataset()

```python
class Dataset(object):
	def  __getitem__(self, index):
		raise NotImplementError
	def __len__(self):
		raise NotImplementError
	def __add__(self, other):
		return ConcatDataset([self, other])

```

torch.utils.data.Dataset表示一个数据集的抽象类，所有的其它数据集都要以它为父类进行数据封装。Dataset的类函数__getitem__和__len__必须要被进行重写

### 2.2 torch.utils.data.TensorDataset()

```python
torch.utils.data.TensorDataset(data_tensor, target_tensor)
        data_tensor : 需要被封装的数据样本
        target_tensor : 需要被封装的数据标签
```

```python
class TensorDataset(Dataset):
    # TensorDataset继承Dataset, 重载了__init__, __getitem__, __len__
    def __init__(self, data_tensor, target_tensor):
        self.data_tensor = data_tensor
        self.target_tensor = target_tensor
    def __getitem__(self, index):
        return self.data_tensor[index], self.target_tensor[index]
    def __len__(self):
        return self.data_tensor.size(0)

```

torch.utils.data.TensorDataset继承父类torch.utils.data.Dataset，不需要对类TensorDataset的函数进行重写

### 2.3 torch.utils.data.DataLoader()

```python
class torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, multiprocessing_context=None)
```

```python
dataset (Dataset): 封装后的数据集。
batch_size (python:int,optional)): 每一批加载的样本量，默认值为1。
shuffle (bool,optional): 设置为True时,每一个epoch重新打乱数据顺序。
sampler (Sampler,optional): 定义在数据集中进行采样的策略，如果被指定，则False必须为shuffle。
batch_sampler (Sampler,optional): 类似sampler，但是一次返回一批索引。互斥有batch_size，shuffle，sampler和drop_last。
num_workers (python:int,optional): 多少个子进程用于数据加载。0表示将在主进程中加载数据,默认值为0。
collate_fn(callable,optional): 合并样本列表以形成张量的小批量。在从地图样式数据集中使用批量加载时使用。
pin_memory (bool,optional): 如果为True，则数据加载器在将张量返回之前将其复制到CUDA固定的内存中。
drop_last (bool,optional): 设置为True，如果数据集大小不能被该批次大小整除则删除最后一个不完整的批次。如果False，数据集的大小不能被批量大小整除，那么最后一个批量将更小，默认值为False。
timeout (numeric,optional): 如果为正，则为从worker收集批次的超时值。应始终为非负数,默认值为0。
worker_init_fn (callable,optional): 如果不是None，则在种子工作之后和数据加载之前，将在每个工作程序子进程上调用此程序，并以工作程序ID作为输入,取值为[0, num_workers - 1]或None。
```

torch.utils.data.DataLoader结合了数据集和取样器，并且可以提供多个线程处理数据集。在训练模型时该类可以将数据进行切分，每次抛出一组数据，直至把所有的数据都抛出

### 2.4 torchvision.datasets.ImageFolder()

文件夹的名称为target，对应的文件夹中内容为数据

```python
dataset=torchvision.datasets.ImageFolder(
                       root, transform=None, 
                       target_transform=None, 
                       loader=<function default_loader>, 
                       is_valid_file=None)
```

- root：图片存储的根目录，即各类别文件夹所在目录的上一级目录。
- transform：对图片进行预处理的操作（函数），原始图片作为输入，返回一个转换后的图片。
- target_transform：对图片类别进行预处理的操作，输入为 target，输出对其的转换。 如果不传该参数，即对 target 不做任何转换，返回的顺序索引 0,1, 2…
- loader：表示数据集加载方式，通常默认加载方式即可。
- is_valid_file：获取图像文件的路径并检查该文件是否为有效文件的函数(用于检查损坏文件)



### 2.5 transforms.ToTensor(）

ToTensor()将shape为`(H, W, C)`的nump.ndarray或img转为shape为`(C, H, W)`的tensor，其将每一个数值归一化到**[0,1]**，其归一化方法比较简单，直接除以255即可

### 2.6 transforms.Normalize()

<img src="https://drailife.oss-cn-beijing.aliyuncs.com/img/202207162359225.png"></img>

```python
transforms.Compose([transforms.ToTensor(),
                    transforms.Normalize(std=(0.5,0.5,0.5),mean=(0.5,0.5,0.5))])
# 则其作用就是先将输入归一化到(0,1)，再使用公式"(x-mean)/std"，将每个元素分布到(-1,1)
```

mean 和 std取值可以总数据中抽样进行计算

### 2.7 transforms.Compose()

一般用Compose将多个处理步骤整合到一起

```python
transform = transforms.Compose([ 
            transforms.ToTensor(), 
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), 
        ]) 

```



### 2.8 加载自己的数据集

```python
class MyDataset(Dataset):
	def __init__(self, root, transform=None):
    	pass
        def __getitem__(self, index):
            # 	倾向于在这里调用transform
    	pass		# 可以使用下标访问
        def __len__(self):
    	pass		# 返回数据集大小
transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize(mean=(0.5,), std=(0.5,))
                                ])
dataset = MyDataset(root='your path', transform=transform)
train_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=2)
# 若使用num_workers 在windows系统下需要加上if __name__ == '__main__':
# 在数据集较小的时候速度会变慢
```

## 第一种标签在文件夹上的数据加载方法



```python
import torchvision.datasets
import torchvision.transforms as transforms

path = '..//3.MyDataSet//Archive//seg_train'
transform = transforms.Compose([
 transforms.Resize((224,224)), # 图片像素改为224*224
 transforms.ToTensor(), # 将图片(Image)转成Tensor，归一化至[0, 1]
 transforms.Normalize(mean=[0.492, 0.461, 0.417], std=[0.256, 0.248, 0.251]) # 标准化至[-1, 1]，规定均值和标准差
])
data_set = torchvision.datasets.ImageFolder(root=path, transform=transform)
data_loader = torch.utils.data.DataLoader(dataset=data_set, batch_size=64, shuffle=True)
```



##  第二种标签在图片名上的数据加载方法

![image-20220716235304591](https://drailife.oss-cn-beijing.aliyuncs.com/img/202207162353458.png)



```python
    def __init__(self, path_dir, transform=None): #初始化一些属性
        self.path_dir = path_dir #文件路径
        self.transform = transform #对图形进行处理，如标准化、截取、转换等
        self.images = os.listdir(self.path_dir)#把路径下的所有文件放在一个列表中

    def __len__(self):#返回整个数据集的大小
        return len(self.images)

    def __getitem__(self,index):#根据索引index返回图像及标签
        image_index = self.images[index]#根据索引获取图像文件名称
        img_path = os.path.join(self.path_dir, image_index)#获取图像的路径或目录
        img = Image.open(img_path).convert('RGB')# 读取图像

        # 根据目录名称获取图像标签（cat或dog）
        label = img_path.split('\\')[-1].split('.')[0]
        #把字符转换为数字cat-0，dog-1
        label = 1 if 'dog' in label else 0

        if self.transform is not None:
            img = self.transform(img)
        return img,label
```



## csv文件存储标签的数据集的加载的方法

根据csv中的标签创建相应的文件夹存储文件随后用 ImageFolder 方式读取



## 将数据集分为训练集，验证集和测试集的方法

训练集train_data和验证集val_data加载数据时，需要将第二个参数train设置为对应的值，便可以获取对应的数据

```python
class MyDataset(Dataset): #继承Dataset
    def __init__(self, root, transforms=None, train=True, test=False): #初始化一些属性
        self.test = test#将test变量的赋值
        imgs = [os.path.join(root, img) for img in os.listdir(root)]#数据的路径列表
        if self.test:
            imgs = sorted(imgs, key=lambda x: int(x.split('.')[-2].split('\\')[-1]))
            #对测试集的数据进行排序
        else:
            imgs = sorted(imgs, key=lambda x: int(x.split('.')[-2]))
            #对非测试集的数据进行排序
            #排序的目的是便于后续的分割
        imgs_num = len(imgs)#获取数据的长度便于切分数据集
        if self.test:
            self.imgs = imgs#将测试集的数据直接导入
        elif train:
            self.imgs = imgs[:int(0.7 * imgs_num)]#将train中数据的70%给train
        else:
            self.imgs = imgs[int(0.7 * imgs_num):]#剩下的30%做验证集
    
        if transforms is None:#对数据进行增强处理
            normalize = T.Normalize(mean=[0.488,0.455,0.417],
                                    std=[0.261,0.255,0.257])
 
            if self.test or not train:
                self.transforms = T.Compose([
                    T.Resize(28),
                    T.CenterCrop(28),
                    T.ToTensor(),
                    normalize
                ])
            else:
                self.transforms = T.Compose([
                    T.Resize(28),
                    T.CenterCrop(28),
                    T.RandomHorizontalFlip(),
                    T.ToTensor(),
                    normalize
                ])

    def __len__(self):#返回整个数据集的大小
        return len(self.imgs)

    def __getitem__(self,index):#根据索引index返回图像及标签
        img_path = self.imgs[index]
        if self.test:
            label = int(self.imgs[index].split('.')[-2].split('\\')[-1])
            #获取测试集文件名的部分作为标签
        else:
            label = 1 if 'dog' in img_path.split('\\')[-1] else 0
            #获取train中文件名中的标签并进行数字化，dog为1，cat为0
        data = Image.open(img_path)
        data = self.transforms(data)
        return data, label
```



## 计算图片数据集的均值和方差

当要对图片数据集进行归一化时，需要计算它的均值和方差

把图片转换为张量，用ToTenser()转换，[遍历](https://so.csdn.net/so/search?q=遍历&spm=1001.2101.3001.7020)每一张图片，接着遍历每一张图片的RGB通道，计算每个通道的均值和方差，最后使每一通道的均值和方差除以图片总数，就得出所要的归一化均值和方差



```python
import numpy as np
import torchvision.datasets
import torchvision.transforms as transforms
import tqdm

path = '..//3.MyDataSet//Archive//seg_train'
transform = transforms.ToTensor()
data_set = torchvision.datasets.ImageFolder(root=path, transform=transform)
num_imgs = len(data_set)  # 获取数据集的图片数量
means, stds = [0, 0, 0], [0, 0, 0]  # 初始化均值和方差
data_bar = tqdm.tqdm(data_set)
for i, (imgs, lables) in enumerate(data_bar):
    for channel in range(3):  # 遍 历图片的RGB三通道
        # 计算每一个通道的均值和标准差
        means[channel] += imgs[channel, :, :].mean()
        stds[channel] += imgs[channel, :, :].std()
    data_bar.set_description(f"[{i}/{num_imgs}]")
mean = np.array(means) / num_imgs
std = np.array(stds) / num_imgs  # 要使数据集归一化，均值和方差需除以总图片数量
print(mean, std)  # 打印出结果

```

